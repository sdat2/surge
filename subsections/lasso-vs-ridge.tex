\subsection{Linear ML models}
\label{sec:lin-ml-models}

To regress some set of inputs, $X$, against
some set of targets, $Y$,
we need to choose an objective function to minimise:

\begin{align}
J_{\mathrm{mlr}} = & \|X w-y\|_{2}^{2} \tag{MLR}, \label{eq:MLR} \\
J_{\mathrm{lasso}} = &
\frac{1}{2 n_{\text {samples }}}\|X w-y\|_{2}^{2}+\alpha\|w\|_{1} \tag{LAS}, \label{eq:LAS} \\
J_{\mathrm{ridge}} = &  \|X w-y\|_{2}^{2}+\alpha\|w\|_{2}^{2} \tag{RID}, \label{eq:RID}
\end{align}

Where \ref{eq:MLR} is the ordinary minimisation of least squares, that has no
penalty for the complexity of parameters.
The lasso objective function adds a penalty~\ref{eq:LAS},
$\alpha\|w\|_{1}$ added, where $\alpha$ is a constant and $\|w\|_{1}$ is the
$l_1$-norm of the coefficient vector.
The ridge objective function~\ref{eq:RID}
$\alpha\ge0$ controls the amount of shrinkage:
the larger value of $\alpha$,
the greater the amount of shrinkage
and thus the coefficients become more robust to multi-colinearity.
A coordinate descent algorithm is used to~\cite{scikit-learn}
$
\min _{w} (J)
$.

The complexity parameter $\alpha$, can be set to
the value that leads to the greatest generalisability to unseen data.
